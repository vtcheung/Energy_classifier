{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdFIg7HiFLvn"
      },
      "source": [
        "# Energy Based Classification for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3nqsSYMFV74"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFfQp0dVGKn9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z989poLZFkhW"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()  # not no_cuda and\n",
        "batch_size = 100 #100\n",
        "test_batch_size = 1000\n",
        "lr = 0.02\n",
        "gamma = 0.8 #0.8\n",
        "epochs = 10\n",
        "seed = np.random.randint(0, 1000)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "out = 1\n",
        "n_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9XFJTKSF7Pv"
      },
      "outputs": [],
      "source": [
        "class H1(nn.Module):\n",
        "    # Hamiltonian neural network, as presented in [1,2].\n",
        "    # H_1-DNN and H_2-DNN\n",
        "    # General ODE: \\dot{y} = J(y,t) K(t) \\tanh( K^T(t) y(t) + b(t) )\n",
        "    # Constraints:\n",
        "    #   J(y,t) = J_1 = [ 0 I ; -I 0 ]  or  J(y,t) = J_2 = [ 0 1 .. 1 ; -1 0 .. 1 ; .. ; -1 -1 .. 0 ].\n",
        "    # Discretization method: Forward Euler\n",
        "    def __init__(self, n_layers, t_end, nf, random=True, select_j='J1',n_classes=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers  # nt: number of layers\n",
        "        self.h = t_end / self.n_layers  #interval\n",
        "        self.act = nn.Tanh()    # activation function\n",
        "        self.nf = nf            # number of features\n",
        "        self.n_classes = n_classes # number of classes\n",
        "\n",
        "        if random:\n",
        "            K = torch.randn(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.randn(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.randn(self.nf, self.n_classes,1)\n",
        "            final_b = torch.randn(self.n_classes, 1, 1)\n",
        "        else:\n",
        "            K = torch.ones(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.zeros(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.ones(self.nf, self.n_classes, 1)\n",
        "            final_b = torch.zeros(self.n_classes, 1, 1)\n",
        "        \n",
        "        self.K = nn.Parameter(K, True)\n",
        "        self.b = nn.Parameter(b, True)\n",
        "        self.final_K = nn.Parameter(final_K, True)\n",
        "        self.final_b = nn.Parameter(final_b, True)\n",
        "\n",
        "        if select_j == 'J1':\n",
        "            j_identity = torch.eye(self.nf//2)\n",
        "            j_zeros = torch.zeros(self.nf//2, self.nf//2)\n",
        "            self.J = torch.cat((torch.cat((j_zeros, j_identity), 0), torch.cat((- j_identity, j_zeros), 0)), 1)\n",
        "        else:\n",
        "            j_aux = np.hstack((np.zeros(1), np.ones(self.nf-1)))\n",
        "            J = j_aux\n",
        "            for j in range(self.nf-1):\n",
        "                j_aux = np.hstack((-1 * np.ones(1), j_aux[:-1]))\n",
        "                J = np.vstack((J, j_aux))\n",
        "            self.J = torch.tensor(J, dtype=torch.float32)\n",
        "\n",
        "    def getK(self):\n",
        "        return self.K\n",
        "\n",
        "    def getb(self):\n",
        "        return self.b\n",
        "\n",
        "    def getJ(self):\n",
        "        return self.J\n",
        "\n",
        "    def forward(self, Y0, ini=0, end=None):\n",
        "\n",
        "        dim = len(Y0.shape)\n",
        "        Y = Y0.transpose(1, dim-1)\n",
        "\n",
        "        if end is None:\n",
        "            end = self.n_layers\n",
        "        \n",
        "        for j in range(ini, end-1):\n",
        "            Y = Y + self.h * F.linear(self.act(F.linear(\n",
        "                Y, self.K[:, :, j].transpose(0, 1), self.b[:, 0, j])), torch.matmul(self.J, self.K[:, :, j]))\n",
        "            \n",
        "        NNoutput = Y.transpose(1, dim-1)\n",
        "\n",
        "        return NNoutput, self.K, self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbjV_8yD7Gip"
      },
      "outputs": [],
      "source": [
        "def logcos(x):\n",
        "    return x + torch.log(1+torch.exp(-2.0*x)) - torch.log(torch.tensor(2.0))\n",
        "    # return torch.abs(x) + torch.log(1+torch.exp(-2.0*torch.abs(x))) - torch.log(torch.tensor(2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0AIZQjHML3U"
      },
      "outputs": [],
      "source": [
        "def compute_H(y,K,b):\n",
        "    dim = len(y.shape)\n",
        "    y = y.transpose(1, dim-1)\n",
        "    n_layers = K.shape[-1]\n",
        "    H = torch.sum(logcos(F.linear(\n",
        "                y.squeeze(2), K[:, :, n_layers-1].transpose(0, 1), b[:, 0, n_layers-1])),dim-1)\n",
        "    return H"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_H(y,K,b):\n",
        "  H_ALL = []\n",
        "  dim = len(y.shape)\n",
        "  y = y.transpose(1, dim-1)\n",
        "  n_layers = K.shape[-1]\n",
        "  for layer in range(n_layers):\n",
        "    H_ALL.append(torch.sum(torch.log(torch.cosh(F.linear(\n",
        "                y.squeeze(2), K[:, :, layer-1].transpose(0, 1), b[:, 0, layer-1]))),dim-1))\n",
        "  \n",
        "  return H_ALL"
      ],
      "metadata": {
        "id": "yzop1wGMN620"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk3dTxmeGoWn"
      },
      "outputs": [],
      "source": [
        "class Net_Energy(nn.Module):\n",
        "    def __init__(self, nf=8, n_layers=4, h=0.5, net_type='H1_J1'):\n",
        "        super(Net_Energy, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=nf, kernel_size=3, stride=1, padding=1)\n",
        "        self.hamiltonian = H1(n_layers=n_layers, t_end=h * n_layers, nf=nf, select_j='J1')\n",
        "        self.fc_end = nn.Linear(28*28,10)\n",
        "        self.nf = nf\n",
        "        self.H = []\n",
        "        self.H_ALL = []\n",
        "    \n",
        "    def get_all_H(self):\n",
        "        return self.H_ALL\n",
        "\n",
        "    def getH(self):\n",
        "        return self.H\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x,K,b = self.hamiltonian(x)\n",
        "        self.H = compute_H(x,K,b)\n",
        "        self.H_ALL = compute_all_H(x,K,b)\n",
        "        x = self.H.reshape(-1,28*28)\n",
        "        output = self.fc_end(x)\n",
        "        # output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zfCdxpzL9n1",
        "outputId": "c0df9847-7a60-4c1d-9d87-d603c938b2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------------\n",
            "MNIST dataset - H1_J1-DNN - 4 layers\n",
            "== sgd with Adam (lr=2.0e-02, weight_decay=4.0e-03, gamma=0.8, max_epochs=10, alpha=8.0e-03, minibatch=100)\n"
          ]
        }
      ],
      "source": [
        "# Define the net model\n",
        "n_layers = 4\n",
        "net_type = 'H1_J1'\n",
        "\n",
        "h = 0.4\n",
        "wd = 4e-3\n",
        "alpha = 8e-3\n",
        "alpha_H = 1e-5 #1e-5\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 20, 'pin_memory': True} if use_cuda else {}\n",
        "model = Net_Energy(nf=8, n_layers=n_layers, h=h, net_type=net_type).to(device)\n",
        "\n",
        "print(\"\\n------------------------------------------------------------------\")\n",
        "print(\"MNIST dataset - %s-DNN - %i layers\" % (net_type, n_layers))\n",
        "print(\"== sgd with Adam (lr=%.1e, weight_decay=%.1e, gamma=%.1f, max_epochs=%i, alpha=%.1e, minibatch=%i)\" %\n",
        "      (lr, wd, gamma, epochs, alpha, batch_size))\n",
        "\n",
        "best_acc = 0\n",
        "best_acc_train = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMbuFI3yNYRb"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# Load test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=test_batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT8z2cw_NffS"
      },
      "outputs": [],
      "source": [
        "# Define optimization algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# Scheduler for learning_rate parameter\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEEi5eIgX2Og"
      },
      "outputs": [],
      "source": [
        "def regularization(alpha, h, K, b, H_ALL, alpha_H):\n",
        "    # Regularization function as introduced in [1]\n",
        "    n_layers = K.shape[-1]\n",
        "    loss = 0\n",
        "    for j in range(n_layers - 1):\n",
        "        loss = loss + alpha * h * (1 / 2 * torch.norm(K[:, :, j + 1] - K[:, :, j]) ** 2 +\n",
        "                                   1 / 2 * torch.norm(b[:, :, j + 1] - b[:, :, j]) ** 2) + alpha_H * h *  1 / 2 * torch.norm(H_ALL[j+1] - H_ALL[j]) ** 2\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Y9Ega3YSxI"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, alpha, out, alpha_H):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        K = model.hamiltonian.getK()\n",
        "        b = model.hamiltonian.getb()\n",
        "        H_ALL = model.get_all_H()\n",
        "        for j in range(int(model.hamiltonian.n_layers) - 1):\n",
        "            loss = loss + regularization(alpha, h, K, b, H_ALL, alpha_H)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0 and out>0:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "            print('\\tTrain Epoch: {:2d} [{:5d}/{} ({:2.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), correct, len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnIKX7A1YUbW"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader, out):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if out > 0:\n",
        "        print('Test set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCciasJnYFMm",
        "outputId": "a8125d8f-c52a-47ba-c1e6-802f473bce71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch:  1 [    0/60000 ( 0%)]\tLoss: 113.971863\tAccuracy: 10/100\n",
            "\tTrain Epoch:  1 [10000/60000 (17%)]\tLoss: 2.893197\tAccuracy: 78/100\n",
            "\tTrain Epoch:  1 [20000/60000 (33%)]\tLoss: 2.178260\tAccuracy: 83/100\n",
            "\tTrain Epoch:  1 [30000/60000 (50%)]\tLoss: 1.631594\tAccuracy: 87/100\n",
            "\tTrain Epoch:  1 [40000/60000 (67%)]\tLoss: 1.737917\tAccuracy: 89/100\n",
            "\tTrain Epoch:  1 [50000/60000 (83%)]\tLoss: 1.316394\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.4059, Accuracy:  8804/10000 (88.04%)\n",
            "Train set:\tAverage loss: 0.4212, Accuracy: 52440/60000 (87.40%)\n",
            "\tTrain Epoch:  2 [    0/60000 ( 0%)]\tLoss: 1.428914\tAccuracy: 87/100\n",
            "\tTrain Epoch:  2 [10000/60000 (17%)]\tLoss: 1.297026\tAccuracy: 88/100\n",
            "\tTrain Epoch:  2 [20000/60000 (33%)]\tLoss: 1.294579\tAccuracy: 90/100\n",
            "\tTrain Epoch:  2 [30000/60000 (50%)]\tLoss: 1.242957\tAccuracy: 86/100\n",
            "\tTrain Epoch:  2 [40000/60000 (67%)]\tLoss: 1.110102\tAccuracy: 93/100\n",
            "\tTrain Epoch:  2 [50000/60000 (83%)]\tLoss: 1.254778\tAccuracy: 89/100\n",
            "Test set:\tAverage loss: 0.4719, Accuracy:  8516/10000 (85.16%)\n",
            "Train set:\tAverage loss: 0.4823, Accuracy: 50880/60000 (84.80%)\n",
            "\tTrain Epoch:  3 [    0/60000 ( 0%)]\tLoss: 1.105793\tAccuracy: 94/100\n",
            "\tTrain Epoch:  3 [10000/60000 (17%)]\tLoss: 1.085288\tAccuracy: 93/100\n",
            "\tTrain Epoch:  3 [20000/60000 (33%)]\tLoss: 0.891076\tAccuracy: 95/100\n",
            "\tTrain Epoch:  3 [30000/60000 (50%)]\tLoss: 0.950445\tAccuracy: 93/100\n",
            "\tTrain Epoch:  3 [40000/60000 (67%)]\tLoss: 0.915859\tAccuracy: 94/100\n",
            "\tTrain Epoch:  3 [50000/60000 (83%)]\tLoss: 1.077883\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.3056, Accuracy:  9077/10000 (90.77%)\n",
            "Train set:\tAverage loss: 0.3080, Accuracy: 54350/60000 (90.58%)\n",
            "\tTrain Epoch:  4 [    0/60000 ( 0%)]\tLoss: 0.818185\tAccuracy: 94/100\n",
            "\tTrain Epoch:  4 [10000/60000 (17%)]\tLoss: 0.859411\tAccuracy: 93/100\n",
            "\tTrain Epoch:  4 [20000/60000 (33%)]\tLoss: 0.998294\tAccuracy: 90/100\n",
            "\tTrain Epoch:  4 [30000/60000 (50%)]\tLoss: 0.835453\tAccuracy: 93/100\n",
            "\tTrain Epoch:  4 [40000/60000 (67%)]\tLoss: 0.870041\tAccuracy: 92/100\n",
            "\tTrain Epoch:  4 [50000/60000 (83%)]\tLoss: 0.774722\tAccuracy: 94/100\n",
            "Test set:\tAverage loss: 0.2757, Accuracy:  9170/10000 (91.70%)\n",
            "Train set:\tAverage loss: 0.2672, Accuracy: 55260/60000 (92.10%)\n",
            "\tTrain Epoch:  5 [    0/60000 ( 0%)]\tLoss: 0.641880\tAccuracy: 94/100\n",
            "\tTrain Epoch:  5 [10000/60000 (17%)]\tLoss: 0.642125\tAccuracy: 95/100\n",
            "\tTrain Epoch:  5 [20000/60000 (33%)]\tLoss: 0.808268\tAccuracy: 92/100\n",
            "\tTrain Epoch:  5 [30000/60000 (50%)]\tLoss: 0.705731\tAccuracy: 91/100\n",
            "\tTrain Epoch:  5 [40000/60000 (67%)]\tLoss: 0.722986\tAccuracy: 90/100\n",
            "\tTrain Epoch:  5 [50000/60000 (83%)]\tLoss: 0.630682\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.2887, Accuracy:  9113/10000 (91.13%)\n",
            "Train set:\tAverage loss: 0.2763, Accuracy: 55078/60000 (91.80%)\n",
            "\tTrain Epoch:  6 [    0/60000 ( 0%)]\tLoss: 0.520645\tAccuracy: 95/100\n",
            "\tTrain Epoch:  6 [10000/60000 (17%)]\tLoss: 0.558118\tAccuracy: 96/100\n",
            "\tTrain Epoch:  6 [20000/60000 (33%)]\tLoss: 0.711064\tAccuracy: 90/100\n",
            "\tTrain Epoch:  6 [30000/60000 (50%)]\tLoss: 0.681590\tAccuracy: 91/100\n",
            "\tTrain Epoch:  6 [40000/60000 (67%)]\tLoss: 0.639742\tAccuracy: 93/100\n",
            "\tTrain Epoch:  6 [50000/60000 (83%)]\tLoss: 0.706172\tAccuracy: 87/100\n",
            "Test set:\tAverage loss: 0.2759, Accuracy:  9187/10000 (91.87%)\n",
            "Train set:\tAverage loss: 0.2687, Accuracy: 55255/60000 (92.09%)\n",
            "\tTrain Epoch:  7 [    0/60000 ( 0%)]\tLoss: 0.525975\tAccuracy: 92/100\n",
            "\tTrain Epoch:  7 [10000/60000 (17%)]\tLoss: 0.442517\tAccuracy: 92/100\n",
            "\tTrain Epoch:  7 [20000/60000 (33%)]\tLoss: 0.557139\tAccuracy: 90/100\n",
            "\tTrain Epoch:  7 [30000/60000 (50%)]\tLoss: 0.658551\tAccuracy: 93/100\n",
            "\tTrain Epoch:  7 [40000/60000 (67%)]\tLoss: 0.573232\tAccuracy: 93/100\n",
            "\tTrain Epoch:  7 [50000/60000 (83%)]\tLoss: 0.583564\tAccuracy: 90/100\n",
            "Test set:\tAverage loss: 0.3039, Accuracy:  9101/10000 (91.01%)\n",
            "Train set:\tAverage loss: 0.2858, Accuracy: 54958/60000 (91.60%)\n",
            "\tTrain Epoch:  8 [    0/60000 ( 0%)]\tLoss: 0.531340\tAccuracy: 94/100\n",
            "\tTrain Epoch:  8 [10000/60000 (17%)]\tLoss: 0.558061\tAccuracy: 91/100\n",
            "\tTrain Epoch:  8 [20000/60000 (33%)]\tLoss: 0.521052\tAccuracy: 94/100\n",
            "\tTrain Epoch:  8 [30000/60000 (50%)]\tLoss: 0.513425\tAccuracy: 93/100\n",
            "\tTrain Epoch:  8 [40000/60000 (67%)]\tLoss: 0.502311\tAccuracy: 92/100\n",
            "\tTrain Epoch:  8 [50000/60000 (83%)]\tLoss: 0.550879\tAccuracy: 89/100\n",
            "Test set:\tAverage loss: 0.2890, Accuracy:  9134/10000 (91.34%)\n",
            "Train set:\tAverage loss: 0.2724, Accuracy: 55108/60000 (91.85%)\n",
            "\tTrain Epoch:  9 [    0/60000 ( 0%)]\tLoss: 0.390119\tAccuracy: 94/100\n",
            "\tTrain Epoch:  9 [10000/60000 (17%)]\tLoss: 0.423936\tAccuracy: 94/100\n",
            "\tTrain Epoch:  9 [20000/60000 (33%)]\tLoss: 0.448627\tAccuracy: 91/100\n",
            "\tTrain Epoch:  9 [30000/60000 (50%)]\tLoss: 0.401761\tAccuracy: 93/100\n",
            "\tTrain Epoch:  9 [40000/60000 (67%)]\tLoss: 0.305523\tAccuracy: 97/100\n",
            "\tTrain Epoch:  9 [50000/60000 (83%)]\tLoss: 0.410205\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.2645, Accuracy:  9226/10000 (92.26%)\n",
            "Train set:\tAverage loss: 0.2503, Accuracy: 55540/60000 (92.57%)\n",
            "\tTrain Epoch: 10 [    0/60000 ( 0%)]\tLoss: 0.342694\tAccuracy: 94/100\n",
            "\tTrain Epoch: 10 [10000/60000 (17%)]\tLoss: 0.540747\tAccuracy: 92/100\n",
            "\tTrain Epoch: 10 [20000/60000 (33%)]\tLoss: 0.315716\tAccuracy: 93/100\n",
            "\tTrain Epoch: 10 [30000/60000 (50%)]\tLoss: 0.409346\tAccuracy: 93/100\n",
            "\tTrain Epoch: 10 [40000/60000 (67%)]\tLoss: 0.349633\tAccuracy: 95/100\n",
            "\tTrain Epoch: 10 [50000/60000 (83%)]\tLoss: 0.453265\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.2648, Accuracy:  9202/10000 (92.02%)\n",
            "Train set:\tAverage loss: 0.2478, Accuracy: 55618/60000 (92.70%)\n",
            "\n",
            "Network trained!\n",
            "Test accuracy: 92.26%  - Train accuracy: 92.567% \n",
            "------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  train(model, device, train_loader, optimizer, epoch, alpha, out, alpha_H)\n",
        "  test_acc = test(model, device, test_loader, out)\n",
        "  # Results over training set after training\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in train_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          train_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "  if out > 0:\n",
        "      print('Train set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "          train_loss, correct, len(train_loader.dataset),\n",
        "          100. * correct / len(train_loader.dataset)))\n",
        "  scheduler.step()\n",
        "  if best_acc < test_acc:\n",
        "      best_acc = test_acc\n",
        "      best_acc_train = correct\n",
        "\n",
        "print(\"\\nNetwork trained!\")\n",
        "print('Test accuracy: {:.2f}%  - Train accuracy: {:.3f}% '.format(\n",
        "      100. * best_acc / len(test_loader.dataset), 100. * best_acc_train / len(train_loader.dataset)))\n",
        "print(\"------------------------------------------------------------------\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'Net_Energy_1e-5.pkl') \n"
      ],
      "metadata": {
        "id": "99oxSbAnI_7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "mnist_energy_regularized.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}