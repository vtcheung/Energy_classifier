{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_hamilton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYPKZoDOmM9I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()  # not no_cuda and\n",
        "batch_size = 100\n",
        "test_batch_size = 1000\n",
        "lr = 0.04\n",
        "gamma = 0.8\n",
        "epochs = 2\n",
        "seed = np.random.randint(0, 1000)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "out = 1\n",
        "n_classes = 10"
      ],
      "metadata": {
        "id": "HG5rzp1hmQMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class H1(nn.Module):\n",
        "    # Hamiltonian neural network, as presented in [1,2].\n",
        "    # H_1-DNN and H_2-DNN\n",
        "    # General ODE: \\dot{y} = J(y,t) K(t) \\tanh( K^T(t) y(t) + b(t) )\n",
        "    # Constraints:\n",
        "    #   J(y,t) = J_1 = [ 0 I ; -I 0 ]  or  J(y,t) = J_2 = [ 0 1 .. 1 ; -1 0 .. 1 ; .. ; -1 -1 .. 0 ].\n",
        "    # Discretization method: Forward Euler\n",
        "    def __init__(self, n_layers, t_end, nf, random=True, select_j='J1',n_classes=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers  # nt: number of layers\n",
        "        self.h = t_end / self.n_layers  #interval\n",
        "        self.act = nn.Tanh()    # activation function\n",
        "        self.nf = nf            # number of features\n",
        "        self.n_classes = n_classes # number of classes\n",
        "\n",
        "        if random:\n",
        "            K = torch.randn(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.randn(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.randn(self.nf, self.n_classes,1)\n",
        "            final_b = torch.randn(self.n_classes, 1, 1)\n",
        "        else:\n",
        "            K = torch.ones(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.zeros(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.ones(self.nf, self.n_classes, 1)\n",
        "            final_b = torch.zeros(self.n_classes, 1, 1)\n",
        "        \n",
        "        self.K = nn.Parameter(K, True)\n",
        "        self.b = nn.Parameter(b, True)\n",
        "        self.final_K = nn.Parameter(final_K, True)\n",
        "        self.final_b = nn.Parameter(final_b, True)\n",
        "\n",
        "        if select_j == 'J1':\n",
        "            j_identity = torch.eye(self.nf//2)\n",
        "            j_zeros = torch.zeros(self.nf//2, self.nf//2)\n",
        "            self.J = torch.cat((torch.cat((j_zeros, j_identity), 0), torch.cat((- j_identity, j_zeros), 0)), 1)\n",
        "        else:\n",
        "            j_aux = np.hstack((np.zeros(1), np.ones(self.nf-1)))\n",
        "            J = j_aux\n",
        "            for j in range(self.nf-1):\n",
        "                j_aux = np.hstack((-1 * np.ones(1), j_aux[:-1]))\n",
        "                J = np.vstack((J, j_aux))\n",
        "            self.J = torch.tensor(J, dtype=torch.float32)\n",
        "\n",
        "    def getK(self):\n",
        "        return self.K\n",
        "\n",
        "    def getb(self):\n",
        "        return self.b\n",
        "\n",
        "    def getJ(self):\n",
        "        return self.J\n",
        "\n",
        "    def forward(self, Y0, ini=0, end=None):\n",
        "\n",
        "        dim = len(Y0.shape)\n",
        "        Y = Y0.transpose(1, dim-1)\n",
        "\n",
        "        if end is None:\n",
        "            end = self.n_layers\n",
        "        \n",
        "        for j in range(ini, end-1):\n",
        "            Y = Y + self.h * F.linear(self.act(F.linear(\n",
        "                Y, self.K[:, :, j].transpose(0, 1), self.b[:, 0, j])), torch.matmul(self.J, self.K[:, :, j]))\n",
        "            \n",
        "        NNoutput = Y.transpose(1, dim-1)\n",
        "\n",
        "        return NNoutput"
      ],
      "metadata": {
        "id": "2jO4b4trmRa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net_HDNN(nn.Module):\n",
        "    def __init__(self, nf=8, n_layers=4, h=0.5, net_type='H1_J1'):\n",
        "        super(Net_HDNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=nf, kernel_size=3, stride=1, padding=1)\n",
        "        if net_type == 'H1_J1':\n",
        "            self.hamiltonian = H1(n_layers=n_layers, t_end=h * n_layers, nf=nf, select_j='J1')\n",
        "        elif net_type == 'H1_J2':\n",
        "            self.hamiltonian = H1(n_layers=n_layers, t_end=h * n_layers, nf=nf, select_j='J2')\n",
        "        else:\n",
        "            raise ValueError(\"%s model is not yet implemented for MNIST\" % net_type)\n",
        "        self.fc_end = nn.Linear(nf*28*28, 10)\n",
        "        self.nf = nf\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.hamiltonian(x)\n",
        "        x = x.reshape(-1, self.nf*28*28)\n",
        "        output = self.fc_end(x)\n",
        "        # output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2c8BDZCOmWau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, alpha, out):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        K = model.hamiltonian.getK()\n",
        "        b = model.hamiltonian.getb()\n",
        "        for j in range(int(model.hamiltonian.n_layers) - 1):\n",
        "            loss = loss + regularization(alpha, h, K, b)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0 and out>0:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "            print('\\tTrain Epoch: {:2d} [{:5d}/{} ({:2.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), correct, len(data)))"
      ],
      "metadata": {
        "id": "qHD7Q6DYmX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, out):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if out > 0:\n",
        "        print('Test set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return correct"
      ],
      "metadata": {
        "id": "_q6CVbwImkIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()  # not no_cuda and\n",
        "batch_size = 100\n",
        "test_batch_size = 1000\n",
        "lr = 0.04\n",
        "gamma = 0.8\n",
        "epochs = 10\n",
        "seed = np.random.randint(0, 1000)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "out = 1"
      ],
      "metadata": {
        "id": "JUBoSFJomkdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the net model\n",
        "n_layers = 4\n",
        "net_type = 'H1_J1'\n",
        "\n",
        "h = 0.5\n",
        "wd = 4e-3\n",
        "alpha = 8e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 20, 'pin_memory': True} if use_cuda else {}\n",
        "model = Net_HDNN(nf=8, n_layers=n_layers, h=h, net_type=net_type).to(device)\n",
        "\n",
        "print(\"\\n------------------------------------------------------------------\")\n",
        "print(\"MNIST dataset - %s-DNN - %i layers\" % (net_type, n_layers))\n",
        "print(\"== sgd with Adam (lr=%.1e, weight_decay=%.1e, gamma=%.1f, max_epochs=%i, alpha=%.1e, minibatch=%i)\" %\n",
        "      (lr, wd, gamma, epochs, alpha, batch_size))\n",
        "\n",
        "best_acc = 0\n",
        "best_acc_train = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i04vNI9impEm",
        "outputId": "b9f08233-000a-4e97-ac84-c9766ea9c73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------------\n",
            "MNIST dataset - H1_J1-DNN - 4 layers\n",
            "== sgd with Adam (lr=4.0e-02, weight_decay=4.0e-03, gamma=0.8, max_epochs=10, alpha=8.0e-03, minibatch=100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# Load test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=test_batch_size, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "uqz91bwIm1z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimization algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# Scheduler for learning_rate parameter\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "metadata": {
        "id": "hReZlysom4Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regularization(alpha, h, K, b):\n",
        "    # Regularization function as introduced in [1]\n",
        "    n_layers = K.shape[-1]\n",
        "    loss = 0\n",
        "    for j in range(n_layers - 1):\n",
        "        loss = loss + alpha * h * (1 / 2 * torch.norm(K[:, :, j + 1] - K[:, :, j]) ** 2 +\n",
        "                                   1 / 2 * torch.norm(b[:, :, j + 1] - b[:, :, j]) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "gF0Xo9ggoE9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  train(model, device, train_loader, optimizer, epoch, alpha, out)\n",
        "  test_acc = test(model, device, test_loader, out)\n",
        "  # Results over training set after training\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in train_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          train_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "  if out > 0:\n",
        "      print('Train set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "          train_loss, correct, len(train_loader.dataset),\n",
        "          100. * correct / len(train_loader.dataset)))\n",
        "  scheduler.step()\n",
        "  if best_acc < test_acc:\n",
        "      best_acc = test_acc\n",
        "      best_acc_train = correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOWfCvgTm5H1",
        "outputId": "5bf57275-93f6-4093-f5cf-10f62b39b983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch:  1 [    0/60000 ( 0%)]\tLoss: 4.829453\tAccuracy: 44/100\n",
            "\tTrain Epoch:  1 [10000/60000 (17%)]\tLoss: 10.653116\tAccuracy: 92/100\n",
            "\tTrain Epoch:  1 [20000/60000 (33%)]\tLoss: 6.182642\tAccuracy: 85/100\n",
            "\tTrain Epoch:  1 [30000/60000 (50%)]\tLoss: 4.309175\tAccuracy: 95/100\n",
            "\tTrain Epoch:  1 [40000/60000 (67%)]\tLoss: 3.762470\tAccuracy: 96/100\n",
            "\tTrain Epoch:  1 [50000/60000 (83%)]\tLoss: 2.991932\tAccuracy: 94/100\n",
            "Test set:\tAverage loss: 1.2511, Accuracy:  9278/10000 (92.78%)\n",
            "Train set:\tAverage loss: 1.1380, Accuracy: 55880/60000 (93.13%)\n",
            "\tTrain Epoch:  2 [    0/60000 ( 0%)]\tLoss: 2.123989\tAccuracy: 97/100\n",
            "\tTrain Epoch:  2 [10000/60000 (17%)]\tLoss: 1.446579\tAccuracy: 99/100\n",
            "\tTrain Epoch:  2 [20000/60000 (33%)]\tLoss: 1.995941\tAccuracy: 96/100\n",
            "\tTrain Epoch:  2 [30000/60000 (50%)]\tLoss: 1.585781\tAccuracy: 97/100\n",
            "\tTrain Epoch:  2 [40000/60000 (67%)]\tLoss: 1.501003\tAccuracy: 94/100\n",
            "\tTrain Epoch:  2 [50000/60000 (83%)]\tLoss: 1.257621\tAccuracy: 96/100\n",
            "Test set:\tAverage loss: 0.4276, Accuracy:  9490/10000 (94.90%)\n",
            "Train set:\tAverage loss: 0.3816, Accuracy: 57004/60000 (95.01%)\n",
            "\tTrain Epoch:  3 [    0/60000 ( 0%)]\tLoss: 1.008120\tAccuracy: 96/100\n",
            "\tTrain Epoch:  3 [10000/60000 (17%)]\tLoss: 1.018831\tAccuracy: 99/100\n",
            "\tTrain Epoch:  3 [20000/60000 (33%)]\tLoss: 1.130157\tAccuracy: 95/100\n",
            "\tTrain Epoch:  3 [30000/60000 (50%)]\tLoss: 0.968783\tAccuracy: 98/100\n",
            "\tTrain Epoch:  3 [40000/60000 (67%)]\tLoss: 1.238981\tAccuracy: 94/100\n",
            "\tTrain Epoch:  3 [50000/60000 (83%)]\tLoss: 0.763798\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.2659, Accuracy:  9380/10000 (93.80%)\n",
            "Train set:\tAverage loss: 0.2412, Accuracy: 56615/60000 (94.36%)\n",
            "\tTrain Epoch:  4 [    0/60000 ( 0%)]\tLoss: 0.810943\tAccuracy: 95/100\n",
            "\tTrain Epoch:  4 [10000/60000 (17%)]\tLoss: 0.561942\tAccuracy: 99/100\n",
            "\tTrain Epoch:  4 [20000/60000 (33%)]\tLoss: 0.711621\tAccuracy: 95/100\n",
            "\tTrain Epoch:  4 [30000/60000 (50%)]\tLoss: 0.576559\tAccuracy: 96/100\n",
            "\tTrain Epoch:  4 [40000/60000 (67%)]\tLoss: 0.593340\tAccuracy: 96/100\n",
            "\tTrain Epoch:  4 [50000/60000 (83%)]\tLoss: 0.724317\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.1802, Accuracy:  9461/10000 (94.61%)\n",
            "Train set:\tAverage loss: 0.1706, Accuracy: 57068/60000 (95.11%)\n",
            "\tTrain Epoch:  5 [    0/60000 ( 0%)]\tLoss: 0.498318\tAccuracy: 97/100\n",
            "\tTrain Epoch:  5 [10000/60000 (17%)]\tLoss: 0.376994\tAccuracy: 100/100\n",
            "\tTrain Epoch:  5 [20000/60000 (33%)]\tLoss: 0.440412\tAccuracy: 99/100\n",
            "\tTrain Epoch:  5 [30000/60000 (50%)]\tLoss: 0.425550\tAccuracy: 97/100\n",
            "\tTrain Epoch:  5 [40000/60000 (67%)]\tLoss: 0.457426\tAccuracy: 95/100\n",
            "\tTrain Epoch:  5 [50000/60000 (83%)]\tLoss: 0.435481\tAccuracy: 96/100\n",
            "Test set:\tAverage loss: 0.1637, Accuracy:  9513/10000 (95.13%)\n",
            "Train set:\tAverage loss: 0.1555, Accuracy: 57200/60000 (95.33%)\n",
            "\tTrain Epoch:  6 [    0/60000 ( 0%)]\tLoss: 0.367396\tAccuracy: 96/100\n",
            "\tTrain Epoch:  6 [10000/60000 (17%)]\tLoss: 0.313861\tAccuracy: 99/100\n",
            "\tTrain Epoch:  6 [20000/60000 (33%)]\tLoss: 0.375178\tAccuracy: 96/100\n",
            "\tTrain Epoch:  6 [30000/60000 (50%)]\tLoss: 0.329085\tAccuracy: 98/100\n",
            "\tTrain Epoch:  6 [40000/60000 (67%)]\tLoss: 0.322204\tAccuracy: 97/100\n",
            "\tTrain Epoch:  6 [50000/60000 (83%)]\tLoss: 0.463811\tAccuracy: 94/100\n",
            "Test set:\tAverage loss: 0.1785, Accuracy:  9461/10000 (94.61%)\n",
            "Train set:\tAverage loss: 0.1697, Accuracy: 56899/60000 (94.83%)\n",
            "\tTrain Epoch:  7 [    0/60000 ( 0%)]\tLoss: 0.318945\tAccuracy: 96/100\n",
            "\tTrain Epoch:  7 [10000/60000 (17%)]\tLoss: 0.334398\tAccuracy: 95/100\n",
            "\tTrain Epoch:  7 [20000/60000 (33%)]\tLoss: 0.277132\tAccuracy: 98/100\n",
            "\tTrain Epoch:  7 [30000/60000 (50%)]\tLoss: 0.349254\tAccuracy: 95/100\n",
            "\tTrain Epoch:  7 [40000/60000 (67%)]\tLoss: 0.272154\tAccuracy: 97/100\n",
            "\tTrain Epoch:  7 [50000/60000 (83%)]\tLoss: 0.257608\tAccuracy: 97/100\n",
            "Test set:\tAverage loss: 0.1473, Accuracy:  9545/10000 (95.45%)\n",
            "Train set:\tAverage loss: 0.1375, Accuracy: 57435/60000 (95.72%)\n",
            "\tTrain Epoch:  8 [    0/60000 ( 0%)]\tLoss: 0.233376\tAccuracy: 98/100\n",
            "\tTrain Epoch:  8 [10000/60000 (17%)]\tLoss: 0.399727\tAccuracy: 97/100\n",
            "\tTrain Epoch:  8 [20000/60000 (33%)]\tLoss: 0.143422\tAccuracy: 100/100\n",
            "\tTrain Epoch:  8 [30000/60000 (50%)]\tLoss: 0.290547\tAccuracy: 97/100\n",
            "\tTrain Epoch:  8 [40000/60000 (67%)]\tLoss: 0.206142\tAccuracy: 98/100\n",
            "\tTrain Epoch:  8 [50000/60000 (83%)]\tLoss: 0.330016\tAccuracy: 96/100\n",
            "Test set:\tAverage loss: 0.1493, Accuracy:  9517/10000 (95.17%)\n",
            "Train set:\tAverage loss: 0.1453, Accuracy: 57295/60000 (95.49%)\n",
            "\tTrain Epoch:  9 [    0/60000 ( 0%)]\tLoss: 0.346026\tAccuracy: 95/100\n",
            "\tTrain Epoch:  9 [10000/60000 (17%)]\tLoss: 0.296061\tAccuracy: 97/100\n",
            "\tTrain Epoch:  9 [20000/60000 (33%)]\tLoss: 0.195948\tAccuracy: 98/100\n",
            "\tTrain Epoch:  9 [30000/60000 (50%)]\tLoss: 0.212445\tAccuracy: 98/100\n",
            "\tTrain Epoch:  9 [40000/60000 (67%)]\tLoss: 0.184016\tAccuracy: 99/100\n",
            "\tTrain Epoch:  9 [50000/60000 (83%)]\tLoss: 0.254779\tAccuracy: 96/100\n",
            "Test set:\tAverage loss: 0.2050, Accuracy:  9387/10000 (93.87%)\n",
            "Train set:\tAverage loss: 0.1973, Accuracy: 56509/60000 (94.18%)\n",
            "\tTrain Epoch: 10 [    0/60000 ( 0%)]\tLoss: 0.264634\tAccuracy: 95/100\n",
            "\tTrain Epoch: 10 [10000/60000 (17%)]\tLoss: 0.183523\tAccuracy: 97/100\n",
            "\tTrain Epoch: 10 [20000/60000 (33%)]\tLoss: 0.312880\tAccuracy: 93/100\n",
            "\tTrain Epoch: 10 [30000/60000 (50%)]\tLoss: 0.302647\tAccuracy: 97/100\n",
            "\tTrain Epoch: 10 [40000/60000 (67%)]\tLoss: 0.199485\tAccuracy: 97/100\n",
            "\tTrain Epoch: 10 [50000/60000 (83%)]\tLoss: 0.209177\tAccuracy: 96/100\n",
            "Test set:\tAverage loss: 0.1538, Accuracy:  9547/10000 (95.47%)\n",
            "Train set:\tAverage loss: 0.1514, Accuracy: 57319/60000 (95.53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNetwork trained!\")\n",
        "print('Test accuracy: {:.2f}%  - Train accuracy: {:.3f}% '.format(\n",
        "      100. * best_acc / len(test_loader.dataset), 100. * best_acc_train / len(train_loader.dataset)))\n",
        "print(\"------------------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyFSN-dm8ml",
        "outputId": "80ae6bb8-01f3-4fc5-fd25-c75166f8fd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Network trained!\n",
            "Test accuracy: 95.47%  - Train accuracy: 95.532% \n",
            "------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'Ham_Net.pkl') "
      ],
      "metadata": {
        "id": "UcpFQYPXLWF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}