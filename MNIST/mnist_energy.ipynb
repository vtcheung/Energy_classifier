{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdFIg7HiFLvn"
      },
      "source": [
        "# Energy Based Classification for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3nqsSYMFV74"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFfQp0dVGKn9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z989poLZFkhW"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()  # not no_cuda and\n",
        "batch_size = 100 #100\n",
        "test_batch_size = 1000\n",
        "lr = 0.02\n",
        "gamma = 0.8 #0.8\n",
        "epochs = 10\n",
        "seed = np.random.randint(0, 1000)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "out = 1\n",
        "n_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9XFJTKSF7Pv"
      },
      "outputs": [],
      "source": [
        "class H1(nn.Module):\n",
        "    # Hamiltonian neural network, as presented in [1,2].\n",
        "    # H_1-DNN and H_2-DNN\n",
        "    # General ODE: \\dot{y} = J(y,t) K(t) \\tanh( K^T(t) y(t) + b(t) )\n",
        "    # Constraints:\n",
        "    #   J(y,t) = J_1 = [ 0 I ; -I 0 ]  or  J(y,t) = J_2 = [ 0 1 .. 1 ; -1 0 .. 1 ; .. ; -1 -1 .. 0 ].\n",
        "    # Discretization method: Forward Euler\n",
        "    def __init__(self, n_layers, t_end, nf, random=True, select_j='J1',n_classes=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers  # nt: number of layers\n",
        "        self.h = t_end / self.n_layers  #interval\n",
        "        self.act = nn.Tanh()    # activation function\n",
        "        self.nf = nf            # number of features\n",
        "        self.n_classes = n_classes # number of classes\n",
        "\n",
        "        if random:\n",
        "            K = torch.randn(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.randn(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.randn(self.nf, self.n_classes,1)\n",
        "            final_b = torch.randn(self.n_classes, 1, 1)\n",
        "        else:\n",
        "            K = torch.ones(self.nf, self.nf, self.n_layers-1)\n",
        "            b = torch.zeros(self.nf, 1, self.n_layers-1)\n",
        "            final_K = torch.ones(self.nf, self.n_classes, 1)\n",
        "            final_b = torch.zeros(self.n_classes, 1, 1)\n",
        "        \n",
        "        self.K = nn.Parameter(K, True)\n",
        "        self.b = nn.Parameter(b, True)\n",
        "        self.final_K = nn.Parameter(final_K, True)\n",
        "        self.final_b = nn.Parameter(final_b, True)\n",
        "\n",
        "        if select_j == 'J1':\n",
        "            j_identity = torch.eye(self.nf//2)\n",
        "            j_zeros = torch.zeros(self.nf//2, self.nf//2)\n",
        "            self.J = torch.cat((torch.cat((j_zeros, j_identity), 0), torch.cat((- j_identity, j_zeros), 0)), 1)\n",
        "        else:\n",
        "            j_aux = np.hstack((np.zeros(1), np.ones(self.nf-1)))\n",
        "            J = j_aux\n",
        "            for j in range(self.nf-1):\n",
        "                j_aux = np.hstack((-1 * np.ones(1), j_aux[:-1]))\n",
        "                J = np.vstack((J, j_aux))\n",
        "            self.J = torch.tensor(J, dtype=torch.float32)\n",
        "\n",
        "    def getK(self):\n",
        "        return self.K\n",
        "\n",
        "    def getb(self):\n",
        "        return self.b\n",
        "\n",
        "    def getJ(self):\n",
        "        return self.J\n",
        "\n",
        "    def forward(self, Y0, ini=0, end=None):\n",
        "\n",
        "        dim = len(Y0.shape)\n",
        "        Y = Y0.transpose(1, dim-1)\n",
        "\n",
        "        if end is None:\n",
        "            end = self.n_layers\n",
        "        \n",
        "        for j in range(ini, end-1):\n",
        "            Y = Y + self.h * F.linear(self.act(F.linear(\n",
        "                Y, self.K[:, :, j].transpose(0, 1), self.b[:, 0, j])), torch.matmul(self.J, self.K[:, :, j]))\n",
        "            \n",
        "        NNoutput = Y.transpose(1, dim-1)\n",
        "\n",
        "        return NNoutput, self.K, self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbjV_8yD7Gip"
      },
      "outputs": [],
      "source": [
        "def logcos(x):\n",
        "    return x + torch.log(1+torch.exp(-2.0*x)) - torch.log(torch.tensor(2.0))\n",
        "    # return torch.abs(x) + torch.log(1+torch.exp(-2.0*torch.abs(x))) - torch.log(torch.tensor(2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0AIZQjHML3U"
      },
      "outputs": [],
      "source": [
        "def compute_H(y,K,b):\n",
        "    dim = len(y.shape)\n",
        "    y = y.transpose(1, dim-1)\n",
        "    n_layers = K.shape[-1]\n",
        "    H = torch.sum(logcos(F.linear(\n",
        "                y.squeeze(2), K[:, :, n_layers-1].transpose(0, 1), b[:, 0, n_layers-1])),dim-1)\n",
        "    return H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk3dTxmeGoWn"
      },
      "outputs": [],
      "source": [
        "class Net_Energy(nn.Module):\n",
        "    def __init__(self, nf=8, n_layers=4, h=0.5, net_type='H1_J1'):\n",
        "        super(Net_Energy, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=nf, kernel_size=3, stride=1, padding=1)\n",
        "        self.hamiltonian = H1(n_layers=n_layers, t_end=h * n_layers, nf=nf, select_j='J1')\n",
        "        self.fc_end = nn.Linear(28*28,10)\n",
        "        self.nf = nf\n",
        "        self.H = []\n",
        "\n",
        "    def getH(self):\n",
        "        return self.H\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x,K,b = self.hamiltonian(x)\n",
        "        self.H = compute_H(x,K,b)\n",
        "        x = self.H.reshape(-1,28*28)\n",
        "        output = self.fc_end(x)\n",
        "        # output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zfCdxpzL9n1",
        "outputId": "da907aa2-fb8c-4131-f2c3-21c6f4c537f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------------\n",
            "MNIST dataset - H1_J1-DNN - 4 layers\n",
            "== sgd with Adam (lr=2.0e-02, weight_decay=4.0e-03, gamma=0.8, max_epochs=10, alpha=8.0e-03, minibatch=100)\n"
          ]
        }
      ],
      "source": [
        "# Define the net model\n",
        "n_layers = 4\n",
        "net_type = 'H1_J1'\n",
        "\n",
        "h = 0.4\n",
        "wd = 4e-3\n",
        "alpha = 8e-3\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 20, 'pin_memory': True} if use_cuda else {}\n",
        "model = Net_Energy(nf=8, n_layers=n_layers, h=h, net_type=net_type).to(device)\n",
        "\n",
        "print(\"\\n------------------------------------------------------------------\")\n",
        "print(\"MNIST dataset - %s-DNN - %i layers\" % (net_type, n_layers))\n",
        "print(\"== sgd with Adam (lr=%.1e, weight_decay=%.1e, gamma=%.1f, max_epochs=%i, alpha=%.1e, minibatch=%i)\" %\n",
        "      (lr, wd, gamma, epochs, alpha, batch_size))\n",
        "\n",
        "best_acc = 0\n",
        "best_acc_train = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMbuFI3yNYRb"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# Load test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=test_batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT8z2cw_NffS"
      },
      "outputs": [],
      "source": [
        "# Define optimization algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# Scheduler for learning_rate parameter\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEEi5eIgX2Og"
      },
      "outputs": [],
      "source": [
        "def regularization(alpha, h, K, b):\n",
        "    # Regularization function as introduced in [1]\n",
        "    n_layers = K.shape[-1]\n",
        "    loss = 0\n",
        "    for j in range(n_layers - 1):\n",
        "        loss = loss + alpha * h * (1 / 2 * torch.norm(K[:, :, j + 1] - K[:, :, j]) ** 2 +\n",
        "                                   1 / 2 * torch.norm(b[:, :, j + 1] - b[:, :, j]) ** 2)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Y9Ega3YSxI"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, alpha, out):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        K = model.hamiltonian.getK()\n",
        "        b = model.hamiltonian.getb()\n",
        "        for j in range(int(model.hamiltonian.n_layers) - 1):\n",
        "            loss = loss + regularization(alpha, h, K, b)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0 and out>0:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "            print('\\tTrain Epoch: {:2d} [{:5d}/{} ({:2.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), correct, len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnIKX7A1YUbW"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader, out):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if out > 0:\n",
        "        print('Test set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCciasJnYFMm",
        "outputId": "bec81688-81e7-42c0-e00b-a3151f6e2276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch:  1 [    0/60000 ( 0%)]\tLoss: 31.664392\tAccuracy: 10/100\n",
            "\tTrain Epoch:  1 [10000/60000 (17%)]\tLoss: 2.777592\tAccuracy: 65/100\n",
            "\tTrain Epoch:  1 [20000/60000 (33%)]\tLoss: 1.705688\tAccuracy: 83/100\n",
            "\tTrain Epoch:  1 [30000/60000 (50%)]\tLoss: 1.135599\tAccuracy: 93/100\n",
            "\tTrain Epoch:  1 [40000/60000 (67%)]\tLoss: 1.377368\tAccuracy: 87/100\n",
            "\tTrain Epoch:  1 [50000/60000 (83%)]\tLoss: 1.087626\tAccuracy: 92/100\n",
            "Test set:\tAverage loss: 0.3290, Accuracy:  9018/10000 (90.18%)\n",
            "Train set:\tAverage loss: 0.3369, Accuracy: 54101/60000 (90.17%)\n",
            "\tTrain Epoch:  2 [    0/60000 ( 0%)]\tLoss: 1.221670\tAccuracy: 88/100\n",
            "\tTrain Epoch:  2 [10000/60000 (17%)]\tLoss: 1.260760\tAccuracy: 89/100\n",
            "\tTrain Epoch:  2 [20000/60000 (33%)]\tLoss: 1.297637\tAccuracy: 89/100\n",
            "\tTrain Epoch:  2 [30000/60000 (50%)]\tLoss: 1.030837\tAccuracy: 93/100\n",
            "\tTrain Epoch:  2 [40000/60000 (67%)]\tLoss: 1.319708\tAccuracy: 88/100\n",
            "\tTrain Epoch:  2 [50000/60000 (83%)]\tLoss: 1.098006\tAccuracy: 93/100\n",
            "Test set:\tAverage loss: 0.2843, Accuracy:  9136/10000 (91.36%)\n",
            "Train set:\tAverage loss: 0.2902, Accuracy: 54822/60000 (91.37%)\n",
            "\tTrain Epoch:  3 [    0/60000 ( 0%)]\tLoss: 1.001286\tAccuracy: 94/100\n",
            "\tTrain Epoch:  3 [10000/60000 (17%)]\tLoss: 0.999558\tAccuracy: 95/100\n",
            "\tTrain Epoch:  3 [20000/60000 (33%)]\tLoss: 1.045827\tAccuracy: 91/100\n",
            "\tTrain Epoch:  3 [30000/60000 (50%)]\tLoss: 0.926113\tAccuracy: 93/100\n",
            "\tTrain Epoch:  3 [40000/60000 (67%)]\tLoss: 0.851896\tAccuracy: 95/100\n",
            "\tTrain Epoch:  3 [50000/60000 (83%)]\tLoss: 0.985852\tAccuracy: 93/100\n",
            "Test set:\tAverage loss: 0.2775, Accuracy:  9224/10000 (92.24%)\n",
            "Train set:\tAverage loss: 0.2745, Accuracy: 55212/60000 (92.02%)\n",
            "\tTrain Epoch:  4 [    0/60000 ( 0%)]\tLoss: 0.830959\tAccuracy: 95/100\n",
            "\tTrain Epoch:  4 [10000/60000 (17%)]\tLoss: 0.969976\tAccuracy: 90/100\n",
            "\tTrain Epoch:  4 [20000/60000 (33%)]\tLoss: 0.941581\tAccuracy: 90/100\n",
            "\tTrain Epoch:  4 [30000/60000 (50%)]\tLoss: 0.896342\tAccuracy: 90/100\n",
            "\tTrain Epoch:  4 [40000/60000 (67%)]\tLoss: 0.774199\tAccuracy: 96/100\n",
            "\tTrain Epoch:  4 [50000/60000 (83%)]\tLoss: 0.912638\tAccuracy: 89/100\n",
            "Test set:\tAverage loss: 0.2803, Accuracy:  9183/10000 (91.83%)\n",
            "Train set:\tAverage loss: 0.2707, Accuracy: 55147/60000 (91.91%)\n",
            "\tTrain Epoch:  5 [    0/60000 ( 0%)]\tLoss: 0.820787\tAccuracy: 89/100\n",
            "\tTrain Epoch:  5 [10000/60000 (17%)]\tLoss: 0.904018\tAccuracy: 91/100\n",
            "\tTrain Epoch:  5 [20000/60000 (33%)]\tLoss: 0.660808\tAccuracy: 96/100\n",
            "\tTrain Epoch:  5 [30000/60000 (50%)]\tLoss: 0.713531\tAccuracy: 95/100\n",
            "\tTrain Epoch:  5 [40000/60000 (67%)]\tLoss: 0.808021\tAccuracy: 90/100\n",
            "\tTrain Epoch:  5 [50000/60000 (83%)]\tLoss: 0.673042\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.2773, Accuracy:  9129/10000 (91.29%)\n",
            "Train set:\tAverage loss: 0.2771, Accuracy: 55088/60000 (91.81%)\n",
            "\tTrain Epoch:  6 [    0/60000 ( 0%)]\tLoss: 0.851737\tAccuracy: 88/100\n",
            "\tTrain Epoch:  6 [10000/60000 (17%)]\tLoss: 0.952761\tAccuracy: 90/100\n",
            "\tTrain Epoch:  6 [20000/60000 (33%)]\tLoss: 0.888460\tAccuracy: 93/100\n",
            "\tTrain Epoch:  6 [30000/60000 (50%)]\tLoss: 0.717555\tAccuracy: 93/100\n",
            "\tTrain Epoch:  6 [40000/60000 (67%)]\tLoss: 0.587414\tAccuracy: 92/100\n",
            "\tTrain Epoch:  6 [50000/60000 (83%)]\tLoss: 0.705390\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.2655, Accuracy:  9204/10000 (92.04%)\n",
            "Train set:\tAverage loss: 0.2585, Accuracy: 55309/60000 (92.18%)\n",
            "\tTrain Epoch:  7 [    0/60000 ( 0%)]\tLoss: 0.491634\tAccuracy: 97/100\n",
            "\tTrain Epoch:  7 [10000/60000 (17%)]\tLoss: 0.689790\tAccuracy: 89/100\n",
            "\tTrain Epoch:  7 [20000/60000 (33%)]\tLoss: 0.533197\tAccuracy: 98/100\n",
            "\tTrain Epoch:  7 [30000/60000 (50%)]\tLoss: 0.630638\tAccuracy: 92/100\n",
            "\tTrain Epoch:  7 [40000/60000 (67%)]\tLoss: 0.740338\tAccuracy: 88/100\n",
            "\tTrain Epoch:  7 [50000/60000 (83%)]\tLoss: 0.483300\tAccuracy: 95/100\n",
            "Test set:\tAverage loss: 0.2483, Accuracy:  9288/10000 (92.88%)\n",
            "Train set:\tAverage loss: 0.2477, Accuracy: 55674/60000 (92.79%)\n",
            "\tTrain Epoch:  8 [    0/60000 ( 0%)]\tLoss: 0.543607\tAccuracy: 94/100\n",
            "\tTrain Epoch:  8 [10000/60000 (17%)]\tLoss: 0.623591\tAccuracy: 90/100\n",
            "\tTrain Epoch:  8 [20000/60000 (33%)]\tLoss: 0.756075\tAccuracy: 90/100\n",
            "\tTrain Epoch:  8 [30000/60000 (50%)]\tLoss: 0.452837\tAccuracy: 95/100\n",
            "\tTrain Epoch:  8 [40000/60000 (67%)]\tLoss: 0.524625\tAccuracy: 94/100\n",
            "\tTrain Epoch:  8 [50000/60000 (83%)]\tLoss: 0.618383\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.2402, Accuracy:  9285/10000 (92.85%)\n",
            "Train set:\tAverage loss: 0.2384, Accuracy: 55815/60000 (93.03%)\n",
            "\tTrain Epoch:  9 [    0/60000 ( 0%)]\tLoss: 0.497712\tAccuracy: 93/100\n",
            "\tTrain Epoch:  9 [10000/60000 (17%)]\tLoss: 0.538283\tAccuracy: 92/100\n",
            "\tTrain Epoch:  9 [20000/60000 (33%)]\tLoss: 0.511896\tAccuracy: 94/100\n",
            "\tTrain Epoch:  9 [30000/60000 (50%)]\tLoss: 0.485037\tAccuracy: 95/100\n",
            "\tTrain Epoch:  9 [40000/60000 (67%)]\tLoss: 0.353069\tAccuracy: 98/100\n",
            "\tTrain Epoch:  9 [50000/60000 (83%)]\tLoss: 0.370474\tAccuracy: 97/100\n",
            "Test set:\tAverage loss: 0.2560, Accuracy:  9252/10000 (92.52%)\n",
            "Train set:\tAverage loss: 0.2503, Accuracy: 55516/60000 (92.53%)\n",
            "\tTrain Epoch: 10 [    0/60000 ( 0%)]\tLoss: 0.354682\tAccuracy: 95/100\n",
            "\tTrain Epoch: 10 [10000/60000 (17%)]\tLoss: 0.467412\tAccuracy: 93/100\n",
            "\tTrain Epoch: 10 [20000/60000 (33%)]\tLoss: 0.430375\tAccuracy: 94/100\n",
            "\tTrain Epoch: 10 [30000/60000 (50%)]\tLoss: 0.416765\tAccuracy: 93/100\n",
            "\tTrain Epoch: 10 [40000/60000 (67%)]\tLoss: 0.507594\tAccuracy: 90/100\n",
            "\tTrain Epoch: 10 [50000/60000 (83%)]\tLoss: 0.565137\tAccuracy: 91/100\n",
            "Test set:\tAverage loss: 0.2481, Accuracy:  9286/10000 (92.86%)\n",
            "Train set:\tAverage loss: 0.2395, Accuracy: 55790/60000 (92.98%)\n",
            "\n",
            "Network trained!\n",
            "Test accuracy: 92.88%  - Train accuracy: 92.790% \n",
            "------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  train(model, device, train_loader, optimizer, epoch, alpha, out)\n",
        "  test_acc = test(model, device, test_loader, out)\n",
        "  # Results over training set after training\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in train_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          train_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "  if out > 0:\n",
        "      print('Train set:\\tAverage loss: {:.4f}, Accuracy: {:5d}/{} ({:.2f}%)'.format(\n",
        "          train_loss, correct, len(train_loader.dataset),\n",
        "          100. * correct / len(train_loader.dataset)))\n",
        "  scheduler.step()\n",
        "  if best_acc < test_acc:\n",
        "      best_acc = test_acc\n",
        "      best_acc_train = correct\n",
        "\n",
        "print(\"\\nNetwork trained!\")\n",
        "print('Test accuracy: {:.2f}%  - Train accuracy: {:.3f}% '.format(\n",
        "      100. * best_acc / len(test_loader.dataset), 100. * best_acc_train / len(train_loader.dataset)))\n",
        "print(\"------------------------------------------------------------------\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'Energy_Net.pkl') \n"
      ],
      "metadata": {
        "id": "99oxSbAnI_7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "mnist_energy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}