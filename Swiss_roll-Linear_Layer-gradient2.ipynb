{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db6ebf4-d986-4bd5-b336-ab551c483ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bb3eb-9b5c-4a35-a64e-c01642ef2ee3",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86c00c1-e695-40f0-957f-9f5282877b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swiss_roll(data_size, shuffle=True, nf=2, noise_std=0, input_ch=None):\n",
    "\n",
    "    data2d = torch.zeros(data_size, 2, 1)\n",
    "    label = torch.ones(data_size, 1)\n",
    "    label[math.floor(data_size / 2):, :] = 0\n",
    "\n",
    "    r1 = torch.linspace(0, 1, math.ceil(data_size / 2))\n",
    "    r2 = torch.linspace(0.2, 1.2, math.ceil(data_size / 2))\n",
    "    theta = torch.linspace(0, 4 * math.pi - 4 * math.pi / math.ceil(data_size / 2), math.ceil(data_size / 2))\n",
    "    data2d[0:math.ceil(data_size / 2), 0, 0] = r1 * torch.cos(theta)\n",
    "    data2d[0:math.ceil(data_size / 2), 1, 0] = r1 * torch.sin(theta)\n",
    "    data2d[math.floor(data_size / 2):, 0, 0] = r2 * torch.cos(theta)\n",
    "    data2d[math.floor(data_size / 2):, 1, 0] = r2 * torch.sin(theta)\n",
    "    if noise_std:\n",
    "        for i in range(2):\n",
    "            data2d[:, i, 0] = data2d[:, i, 0] + noise_std*torch.randn(data_size)\n",
    "\n",
    "    if shuffle:\n",
    "        data2d, label = _data_shuffle(data2d, label)\n",
    "    \n",
    "    if nf != 2:\n",
    "        data2d = _data_extension(data2d, nf, input_ch)\n",
    "    \n",
    "    domain = [-1.2, 1.2, -1.2, 1.2]\n",
    "    return data2d, label, domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca31d982-cad3-4c30-b417-e68df5faf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 8000\n",
    "train_data_size = 4000\n",
    "test_data_size = data_size - train_data_size\n",
    "nf = 6\n",
    "data_gen = swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ce3830-0ec8-4433-bb35-38487466f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_shuffle(data2d, label):\n",
    "    data_size = data2d.shape[0]\n",
    "    randindex = torch.randperm(data_size)\n",
    "    data2d = data2d[randindex, :, :]\n",
    "    label = label[randindex, :]\n",
    "    return data2d, label\n",
    "def _data_extension(data2d, nf, input_ch=None):\n",
    "    if nf < 2:\n",
    "        print(\"Dimension not valid\")\n",
    "        return\n",
    "    elif nf % 2 == 1:\n",
    "        print(\"Using odd dimension nf\")\n",
    "    data_size = data2d.shape[0]\n",
    "    if input_ch is not None:\n",
    "        # input_ch is a list of two elements. The elements indicate where the data enters.\n",
    "        idx_x = input_ch[0]\n",
    "        idx_y = input_ch[1]\n",
    "    else:\n",
    "        idx_x = 0\n",
    "        idx_y = nf-1\n",
    "    data2d = torch.cat((torch.zeros(data_size, idx_x-0, 1),\n",
    "                        data2d[:, 0:1, :],\n",
    "                        torch.zeros(data_size, idx_y-idx_x-1, 1),\n",
    "                        data2d[:, 1:2, :],\n",
    "                        torch.zeros(data_size, nf-1-idx_y, 1)), 1)\n",
    "    return data2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec801285-1ea7-4d7b-9a1f-9168a5fbfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2d, labels, domain = data_gen(data_size,nf=nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821fffd3-d087-4e03-8ea3-4dc52d119a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_ids)\n",
    "\n",
    "    def __init__(self, list_ids, data_in, labels):\n",
    "        self.list_ids = list_ids\n",
    "        self.data = data_in\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        idx = self.list_ids[index]\n",
    "\n",
    "        x = self.data[idx, :, :]\n",
    "        y = self.labels[idx, :]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "partition = {'train': range(0, data_size, 2),\n",
    "             'test': range(1, data_size, 2)}\n",
    "training_set = Dataset(partition['train'], data2d, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9190fb00-0264-4bc5-91d1-7d2cce408f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = data.DataLoader(training_set, batch_size=125, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916fc4a-690e-479f-aacb-f53e4e431d91",
   "metadata": {},
   "source": [
    "## Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cd4a15-e098-4523-97d3-c55e9e007171",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = 0.5\n",
    "n_layers = 8\n",
    "h = t_end / n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415342cc-ebf2-43db-ab04-076c48f457e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select training parameters\n",
    "alpha = 5e-4\n",
    "alphac = 1e-4\n",
    "learning_rate = 1e-1 #0.5e-1\n",
    "max_iteration = 150\n",
    "max_in_iteration = 10\n",
    "\n",
    "\n",
    "# define network structure and optimizer\n",
    "batch_size = 125\n",
    "training_set = Dataset(partition['train'], data2d, labels)\n",
    "training_generator = data.DataLoader(training_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465cd4e-7c34-42da-a6a2-923133073335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dae200c-9e42-47d4-96e1-81bc89b6acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H1(nn.Module):\n",
    "    # Hamiltonian neural network, as presented in [1,2].\n",
    "    # H_1-DNN and H_2-DNN\n",
    "    # General ODE: \\dot{y} = J(y,t) K(t) \\tanh( K^T(t) y(t) + b(t) )\n",
    "    # Constraints:\n",
    "    #   J(y,t) = J_1 = [ 0 I ; -I 0 ]  or  J(y,t) = J_2 = [ 0 1 .. 1 ; -1 0 .. 1 ; .. ; -1 -1 .. 0 ].\n",
    "    # Discretization method: Forward Euler\n",
    "    def __init__(self, n_layers, t_end, nf, random=True, select_j='J1'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers  # nt: number of layers\n",
    "        self.h = t_end / self.n_layers  #interval\n",
    "        self.act = nn.Tanh()    # activation function\n",
    "        self.nf = nf            # number of features\n",
    "\n",
    "        if random:\n",
    "            K = torch.randn(self.nf, self.nf, self.n_layers)\n",
    "            b = torch.randn(self.nf, 1, self.n_layers)\n",
    "        else:\n",
    "            K = torch.ones(self.nf, self.nf, self.n_layers)\n",
    "            b = torch.zeros(self.nf, 1, self.n_layers)\n",
    "\n",
    "        self.K = nn.Parameter(K, True)\n",
    "        self.b = nn.Parameter(b, True)\n",
    "\n",
    "        if select_j == 'J1':\n",
    "            j_identity = torch.eye(self.nf//2)\n",
    "            j_zeros = torch.zeros(self.nf//2, self.nf//2)\n",
    "            self.J = torch.cat((torch.cat((j_zeros, j_identity), 0), torch.cat((- j_identity, j_zeros), 0)), 1)\n",
    "        else:\n",
    "            j_aux = np.hstack((np.zeros(1), np.ones(self.nf-1)))\n",
    "            J = j_aux\n",
    "            for j in range(self.nf-1):\n",
    "                j_aux = np.hstack((-1 * np.ones(1), j_aux[:-1]))\n",
    "                J = np.vstack((J, j_aux))\n",
    "            self.J = torch.tensor(J, dtype=torch.float32)\n",
    "\n",
    "    def getK(self):\n",
    "        return self.K\n",
    "\n",
    "    def getb(self):\n",
    "        return self.b\n",
    "\n",
    "    def getJ(self):\n",
    "        return self.J\n",
    "\n",
    "    def forward(self, Y0, ini=0, end=None):\n",
    "\n",
    "        dim = len(Y0.shape)\n",
    "        Y = Y0.transpose(1, dim-1)\n",
    "\n",
    "        if end is None:\n",
    "            end = self.n_layers\n",
    "        \n",
    "        for j in range(ini, end):\n",
    "            Y = Y + self.h * F.linear(self.act(F.linear(\n",
    "                Y, self.K[:, :, j].transpose(0, 1), self.b[:, 0, j])), torch.matmul(self.J, self.K[:, :, j]))\n",
    "            \n",
    "        NNoutput = Y.transpose(1, dim-1)\n",
    "\n",
    "        return NNoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32504e35-c156-4b89-a664-aa35232c560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, nf=2, nout=1):\n",
    "        super().__init__()\n",
    "        self.nout = nout\n",
    "        self.W = nn.Parameter(torch.zeros(self.nout, 1), True)\n",
    "        self.mu = nn.Parameter(torch.zeros(1, self.nout), True)\n",
    "\n",
    "    def forward(self, Y0):\n",
    "        # Y = Y0.transpose(1, 2)\n",
    "        Y = Y0.unsqueeze(1).unsqueeze(2)\n",
    "        NNoutput = F.linear(Y, self.W, self.mu).squeeze(1)\n",
    "        return NNoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98310ad-1aa7-4e23-966e-ebaa5731775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(alpha, h, K, b):\n",
    "    # Regularization function as introduced in [1]\n",
    "    n_layers = K.shape[-1]\n",
    "    loss = 0\n",
    "    for j in range(n_layers - 1):\n",
    "        loss = loss + alpha * h * (1 / 2 * torch.norm(K[:, :, j + 1] - K[:, :, j]) ** 2 +\n",
    "                                   1 / 2 * torch.norm(b[:, :, j + 1] - b[:, :, j]) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fefa5d-741c-44d7-9112-2fe333ec87a3",
   "metadata": {},
   "source": [
    "## Training - Hamilton + Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9130d3dc-d46a-41d4-971f-ce6b0fa5273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H1(n_layers, t_end, nf=nf, select_j='J1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fdee524-b2a9-445f-8b07-f60fdfebb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer_k = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680de33f-b8a9-4bd4-8b7e-6eceb56ca8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_H(y,K,b):\n",
    "    n_layers = K.shape[-1]\n",
    "    H = torch.sum(torch.log(torch.cosh(F.linear(\n",
    "                y.squeeze(2), K[:, :, n_layers-1].transpose(0, 1), b[:, 0, n_layers-1]))),1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6580e48-0676-4f12-a862-a153509db548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_states(model, Y0):\n",
    "    # Y_out N-element list containing the intermediates states. Size of each entry: n_samples * dim2 * dim1\n",
    "    # Y_out[n] = torch.zeros([batch_size, nf, 1]), with n=0,1,..,\n",
    "    Y_out = [Y0]\n",
    "    i = 0\n",
    "    for j in range(model.n_layers):\n",
    "        Y = model.forward(Y_out[j], ini=j, end=j + 1)\n",
    "        Y_out.append(Y)\n",
    "        Y_out[j + 1].retain_grad()\n",
    "    K = model.getK()\n",
    "    b = model.getb()\n",
    "    \n",
    "    return Y_out, K, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97450e35-65f7-45f4-a73e-7c322ba54f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy_with_grad(y, K, b):\n",
    "    H_ALL = []\n",
    "    \n",
    "    n_layers = K.shape[-1]\n",
    "    for layer in range(n_layers):\n",
    "        dim = len(y[layer].shape)\n",
    "        current_state = y[layer]\n",
    "        current_state.retain_grad()\n",
    "        H = torch.sum(torch.log(torch.cosh(F.linear(\n",
    "                current_state.squeeze(2), K[:, :, layer].transpose(0, 1), b[:, 0, layer]))),1)\n",
    "        H_ALL.append(H)\n",
    "        H_ALL[layer].retain_grad()\n",
    "    \n",
    "    return H_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa8bebb-a1e6-4af1-b7e6-bca95c0b7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_info = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4a4361-6262-4a7e-ac54-3909b207f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient_info:\n",
    "        loss_func2 = nn.Identity()\n",
    "        gradients_matrix = np.zeros([int(train_data_size/batch_size) * max_iteration, n_layers])\n",
    "else:\n",
    "        gradients_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e968f6b-795c-4392-bfc4-aa43adb72173",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27192/142946214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH_ALL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     \u001b[0mgradients_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mi_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mlocal_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "is_print = 0\n",
    "for epoch in range(max_iteration):\n",
    "\n",
    "    training_iterator = iter(training_generator)\n",
    "\n",
    "    for i_k in range(int(data2d[partition['train']].size(0) / training_generator.batch_size)):\n",
    "\n",
    "        local_samples, local_labels = next(training_iterator)\n",
    "        \n",
    "        model_c = Classification(nf=nf)\n",
    "        optimizer_w = torch.optim.Adam(model_c.parameters(), lr=learning_rate)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            YN = model(local_samples)\n",
    "        \n",
    "        optimizer_k.zero_grad()\n",
    "        K = model.getK()\n",
    "        b = model.getb()\n",
    "        \n",
    "        for i_w in range(max_in_iteration):  # Inner iteration\n",
    "            optimizer_w.zero_grad()\n",
    "            loss = loss_func(model_c(compute_H(YN,K,b)), local_labels)\n",
    "            loss = loss + alphac * 0.5 * (torch.norm(model_c.W) ** 2 + torch.norm(model_c.mu) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer_w.step()\n",
    "        \n",
    "        loss = loss_func(model_c(compute_H(model(local_samples),K,b)), local_labels)\n",
    "        loss += regularization(alpha, h, K, b)\n",
    "        loss.backward()\n",
    "        li = list(optimizer_k.state)\n",
    "        if not (len(li) == 0):\n",
    "            for ii in range(2):\n",
    "                optimizer_k.state[li[ii]]['step'] = epoch\n",
    "        optimizer_k.step()\n",
    "        \n",
    "        ########### compute gradient ##########\n",
    "        if gradient_info:\n",
    "                local_samples.requires_grad = True\n",
    "                \n",
    "                optimizer_k.zero_grad()\n",
    "                \n",
    "                Y_out,K,b = get_intermediate_states(model, local_samples)\n",
    "                H_ALL = get_energy_with_grad(Y_out, K, b)\n",
    "                # H_ALL = get_intermediate_states(model, local_samples)\n",
    "                # H_ALL = model(local_samples,is_grad=True)\n",
    "                H = H_ALL[-1]\n",
    "                \n",
    "                loss2 = loss_func2(H.sum())\n",
    "                loss2.backward()\n",
    "                \n",
    "                for j in range(n_layers):\n",
    "                    grad = H_ALL[j].grad.numpy().sum(axis=0) / training_generator.batch_size\n",
    "                    gradients_matrix[epoch * int(train_data_size / batch_size)+ i_k, j] = grad\n",
    "                local_samples.requires_grad = False\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model_c = Classification(nf=nf)\n",
    "        optimizer_w = torch.optim.Adam(model_c.parameters(), lr=learning_rate)\n",
    "        with torch.no_grad():\n",
    "            YN = model(local_samples)\n",
    "        \n",
    "        for i_w in range(max_in_iteration):  # Inner iteration\n",
    "            optimizer_w.zero_grad()\n",
    "            loss = loss_func(model_c(compute_H(YN,K,b)), local_labels)\n",
    "            loss = loss + alphac * 0.5 * (torch.norm(model_c.W) ** 2 + torch.norm(model_c.mu) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer_w.step()\n",
    "            acc = (torch.ge(model_c(compute_H(model(local_samples),K,b)), 0) == local_labels).sum().numpy() / batch_size\n",
    "        print('\\tTrain Epoch: {:2d} - Loss: {:.6f} - Accuracy: {:.2f}%'.format(epoch, loss, acc*100))\n",
    "\n",
    "# Train classification layer with all the data\n",
    "\n",
    "# Accuracy results\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_acc = (torch.ge(model_c(compute_H(model(data2d[partition['train'], :, :]),K,b)), 0) == labels[partition['train'], :]\n",
    "                 ).sum().numpy() / train_data_size\n",
    "    test_acc = (torch.ge(model_c(compute_H(model(data2d[partition['test'], :, :]),K,b)), 0) == labels[partition['test'], :]\n",
    "                ).sum().numpy() / test_data_size\n",
    "    \n",
    "print('\\tTrain Accuracy: {:.2f}%'.format(train_acc*100))\n",
    "print('\\tTest Accuracy: {:.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c18d17a-6086-4635-8978-7afb82b13f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-170.89375 ],\n",
       "       [ -29.443474],\n",
       "       [-208.08676 ],\n",
       "       [-109.27009 ],\n",
       "       [ -73.27352 ],\n",
       "       [-285.6217  ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_out[7].grad.numpy().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc138b7-3555-4290-b090-a981dc2fcd2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27192/3019567908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mH_ALL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "H_ALL[6].grad.numpy().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a48ace-af38-42af-9ac1-512ed307f859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_ALL[-1].grad.numpy().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d78b33-a5db-4182-ae85-db90fb784611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
